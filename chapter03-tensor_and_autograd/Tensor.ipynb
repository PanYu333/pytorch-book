{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第三章 PyTorch基础：Tensor和Autograd\n",
    "\n",
    "## 3.1 Tensor\n",
    "\n",
    "Tensor，又名张量，读者可能对这个名词似曾相识，因它不仅在PyTorch中出现过，它也是Theano、TensorFlow、\n",
    "Torch和MxNet中重要的数据结构。关于张量的本质不乏深度的剖析，但从工程角度来讲，可简单地认为它就是一个数组，且支持高效的科学计算。它可以是一个数（标量）、一维数组（向量）、二维数组（矩阵）和更高维的数组（高阶数据）。Tensor和Numpy的ndarrays类似，但PyTorch的tensor支持GPU加速。\n",
    "\n",
    "本节将系统讲解tensor的使用，力求面面俱到，但不会涉及每个函数。对于更多函数及其用法，读者可通过在IPython/Notebook中使用函数名加`?`查看帮助文档，或查阅PyTorch官方文档[^1]。\n",
    "\n",
    "[^1]: http://docs.pytorch.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1+cu116'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's begin\n",
    "from __future__ import print_function\n",
    "import torch  as t\n",
    "t.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3.1.1 基础操作\n",
    "\n",
    "学习过Numpy的读者会对本节内容感到非常熟悉，因tensor的接口有意设计成与Numpy类似，以方便用户使用。但不熟悉Numpy也没关系，本节内容并不要求先掌握Numpy。\n",
    "\n",
    "从接口的角度来讲，对tensor的操作可分为两类：\n",
    "\n",
    "1. `torch.function`，如`torch.save`等。\n",
    "2. 另一类是`tensor.function`，如`tensor.view`等。\n",
    "\n",
    "为方便使用，对tensor的大部分操作同时支持这两类接口，在本书中不做具体区分，如`torch.sum (torch.sum(a, b))`与`tensor.sum (a.sum(b))`功能等价。\n",
    "\n",
    "而从存储的角度来讲，对tensor的操作又可分为两类：\n",
    "\n",
    "1. 不会修改自身的数据，如 `a.add(b)`， 加法的结果会返回一个新的tensor。\n",
    "2. 会修改自身的数据，如 `a.add_(b)`， 加法的结果仍存储在a中，a被修改了。\n",
    "\n",
    "函数名以`_`结尾的都是inplace方式, 即会修改调用者自己的数据，在实际应用中需加以区分。\n",
    "\n",
    "#### 创建Tensor\n",
    "\n",
    "在PyTorch中新建tensor的方法有很多，具体如表3-1所示。\n",
    "\n",
    "表3-1: 常见新建tensor的方法\n",
    "\n",
    "|函数|功能|\n",
    "|:---:|:---:|\n",
    "|Tensor(\\*sizes)|基础构造函数|\n",
    "|tensor(data,)|类似np.array的构造函数|\n",
    "|ones(\\*sizes)|全1Tensor|\n",
    "|zeros(\\*sizes)|全0Tensor|\n",
    "|eye(\\*sizes)|对角线为1，其他为0|\n",
    "|arange(s,e,step|从s到e，步长为step|\n",
    "|linspace(s,e,steps)|从s到e，均匀切分成steps份|\n",
    "|rand/randn(\\*sizes)|均匀/标准分布|\n",
    "|normal(mean,std)/uniform(from,to)|正态分布/均匀分布|\n",
    "|randperm(m)|随机排列|\n",
    "\n",
    "这些创建方法都可以在创建的时候指定数据类型dtype和存放device(cpu/gpu).\n",
    "\n",
    "\n",
    "其中使用`Tensor`函数新建tensor是最复杂多变的方式，它既可以接收一个list，并根据list的数据新建tensor，也能根据指定的形状新建tensor，还能传入其他的tensor，下面举几个例子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 7.5670e-44]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 指定tensor的形状\n",
    "a = t.Tensor(2, 3)\n",
    "a # 数值取决于内存空间的状态，print时候可能overflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [4., 5., 6.]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 用list的数据创建tensor\n",
    "b = t.Tensor([[1,2,3],[4,5,6]])\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.tolist() # 把tensor转为list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tensor.size()`返回`torch.Size`对象，它是tuple的子类，但其使用方式与tuple略有区别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_size = b.size()\n",
    "b_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.numel() # b中元素总个数，2*3，等价于b.nelement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[4.3115e-08, 4.0368e-08, 2.1866e+23],\n",
       "         [1.0486e+21, 4.1197e-11, 4.2330e+21]]),\n",
       " tensor([2., 3.]))"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建一个和b形状一样的tensor\n",
    "c = t.Tensor(b_size)\n",
    "# 创建一个元素为2和3的tensor\n",
    "d = t.Tensor((2, 3))\n",
    "c, d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了`tensor.size()`，还可以利用`tensor.shape`直接查看tensor的形状，`tensor.shape`等价于`tensor.size()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "需要注意的是，`t.Tensor(*sizes)`创建tensor时，系统不会马上分配空间，只是会计算剩余的内存是否足够使用，使用到tensor时才会分配，而其它操作都是在创建完tensor之后马上进行空间分配。其它常用的创建tensor的方法举例如下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.ones(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.zeros(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 3, 5])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.arange(1, 6, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.0000,  5.5000, 10.0000])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.linspace(1, 10, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8142, -1.1165, -0.8040],\n",
       "        [ 0.1258, -2.1965,  0.5791]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.randn(2, 3, device=t.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 3, 0, 2, 1])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.randperm(5) # 长度为5的随机排列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0],\n",
       "        [0, 1, 0]], dtype=torch.int32)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.eye(2, 3, dtype=t.int) # 对角线为1, 不要求行列数一致"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.tensor`是在0.4版本新增加的一个新版本的创建tensor方法，使用的方法，和参数几乎和`np.array`完全一致"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scalar: tensor(3.1416), shape of sclar: torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "scalar = t.tensor(3.14159) \n",
    "print('scalar: %s, shape of sclar: %s' %(scalar, scalar.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector: tensor([1, 2]), shape of vector: torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "vector = t.tensor([1, 2])\n",
    "print('vector: %s, shape of vector: %s' %(vector, vector.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = t.Tensor(1,2) # 注意和t.tensor([1, 2])的区别\n",
    "tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.1000, 1.2000],\n",
       "         [2.2000, 3.1000],\n",
       "         [4.9000, 5.2000]]),\n",
       " torch.Size([3, 2]))"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix = t.tensor([[0.1, 1.2], [2.2, 3.1], [4.9, 5.2]])\n",
    "matrix,matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1111, 0.2222, 0.3333]], dtype=torch.float64)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.tensor([[0.11111, 0.222222, 0.3333333]],\n",
    "                     dtype=t.float64,\n",
    "                     device=t.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([0])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empty_tensor = t.tensor([])\n",
    "empty_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 常用Tensor操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过`tensor.view`方法可以调整tensor的形状，但必须保证调整前后元素总数一致。`view`不会修改自身的数据，返回的新tensor与源tensor共享内存，也即更改其中的一个，另外一个也会跟着改变。在实际应用中可能经常需要添加或减少某一维度，这时候`squeeze`和`unsqueeze`两个函数就派上用场了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [3, 4, 5]])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.arange(0, 6)\n",
    "a.view(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a.view(-1, 3) # 当某一维为-1的时候，会自动计算它的大小\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 3])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.unsqueeze(1) # 注意形状，在第1维（下标从0开始）上增加“１” \n",
    "#等价于 b[:,None]\n",
    "b[:, None].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 3])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.unsqueeze(-2).shape # -2表示倒数第二个维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0, 1, 2],\n",
       "          [3, 4, 5]]]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = b.view(1, 1, 1, 2, 3)\n",
    "c.squeeze(0) # 压缩第0维的“１”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [3, 4, 5]])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.squeeze_() # 把所有维度为“1”的压缩\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0, 100,   2],\n",
       "        [  3,   4,   5]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[1] = 100\n",
    "b # a修改，b作为view之后的，也会跟着修改"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`resize`是另一种可用来调整`size`的方法，但与`view`不同，它可以修改tensor的大小。如果新大小超过了原大小，会自动分配新的内存空间，而如果新大小小于原大小，则之前的数据依旧会被保存，看一个例子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0, 100,   2]])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.resize_(1, 3)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[                  0,                 100,                   2],\n",
       "        [                  3,                   4,                   5],\n",
       "        [7233190455505199148, 9041856110933406817, 8458716092899139628]])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.resize_(3, 3) # 旧的数据依旧保存着，多出的大小会分配新空间\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 索引操作\n",
    "\n",
    "Tensor支持与numpy.ndarray类似的索引操作，语法上也类似，下面通过一些例子，讲解常用的索引操作。如无特殊说明，索引出来的结果与原tensor共享内存，也即修改一个，另一个会跟着修改。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9240,  0.3005,  1.2490, -1.4824],\n",
       "        [ 0.4364,  1.8257, -1.9064,  0.0735],\n",
       "        [-0.1595,  1.7845,  1.0388, -1.0005]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.randn(3, 4)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.9240,  0.3005,  1.2490, -1.4824])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0] # 第0行(下标从0开始)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.9240,  0.4364, -0.1595])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:, 0] # 第0列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.2490)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0][2] # 第0行第2个元素，等价于a[0, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-1.4824)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0, -1] # 第0行最后一个元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9240,  0.3005,  1.2490, -1.4824],\n",
       "        [ 0.4364,  1.8257, -1.9064,  0.0735]])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:2] # 前两行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9240,  0.3005],\n",
       "        [ 0.4364,  1.8257]])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:2, 0:2] # 前两行，第0,1列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.9240,  0.3005]])\n",
      "tensor([-0.9240,  0.3005])\n"
     ]
    }
   ],
   "source": [
    "print(a[0:1, :2]) # 第0行，前两列\n",
    "print(a[0, :2]) # 注意两者的区别：形状不同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 4])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# None类似于np.newaxis, 为a新增了一个轴\n",
    "# 等价于a.view(1, a.shape[0], a.shape[1])\n",
    "a[None].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 4])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[None].shape # 等价于a[None,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 4])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:,None,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 4, 1, 1])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:,None,:,None,None].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False,  True, False],\n",
       "        [False,  True, False, False],\n",
       "        [False,  True,  True, False]])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a > 1 # 返回一个ByteTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.2490, 1.8257, 1.7845, 1.0388])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[a>1] # 等价于a.masked_select(a>1)\n",
    "# 选择结果与原tensor不共享内存空间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9240,  0.3005,  1.2490, -1.4824],\n",
       "        [ 0.4364,  1.8257, -1.9064,  0.0735]])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[t.LongTensor([0,1])] # 第0行和第1行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其它常用的选择函数如表3-2所示。\n",
    "\n",
    "表3-2常用的选择函数\n",
    "\n",
    "函数|功能|\n",
    ":---:|:---:|\n",
    "index_select(input, dim, index)|在指定维度dim上选取，比如选取某些行、某些列\n",
    "masked_select(input, mask)|例子如上，a[a>0]，使用ByteTensor进行选取\n",
    "non_zero(input)|非0元素的下标\n",
    "gather(input, dim, index)|根据index，在dim维度上选取数据，输出的size与index一样\n",
    "\n",
    "\n",
    "`gather`是一个比较复杂的操作，对一个2维tensor，输出的每个元素如下：\n",
    "\n",
    "```python\n",
    "out[i][j] = input[index[i][j]][j]  # dim=0\n",
    "out[i][j] = input[i][index[i][j]]  # dim=1\n",
    "```\n",
    "三维tensor的`gather`操作同理，下面举几个例子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11],\n",
       "        [12, 13, 14, 15]])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.arange(0, 16).view(4, 4)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  5, 10, 15]])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 选取对角线的元素\n",
    "index = t.LongTensor([[0,1,2,3]])\n",
    "a.gather(0, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3],\n",
       "        [ 6],\n",
       "        [ 9],\n",
       "        [12]])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 选取反对角线上的元素\n",
    "index = t.LongTensor([[3,2,1,0]]).t()\n",
    "a.gather(1, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12,  9,  6,  3]])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 选取反对角线上的元素，注意与上面的不同\n",
    "index = t.LongTensor([[3,2,1,0]])\n",
    "a.gather(0, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  3],\n",
       "        [ 5,  6],\n",
       "        [10,  9],\n",
       "        [15, 12]])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 选取两个对角线上的元素\n",
    "index = t.LongTensor([[0,1,2,3],[3,2,1,0]]).t()\n",
    "b = a.gather(1, index)\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与`gather`相对应的逆操作是`scatter_`，`gather`把数据从input中按index取出，而`scatter_`是把取出的数据再放回去。注意`scatter_`函数是inplace操作。\n",
    "\n",
    "```python\n",
    "out = input.gather(dim, index)\n",
    "-->近似逆操作\n",
    "out = Tensor()\n",
    "out.scatter_(dim, index)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  0.,  0.,  3.],\n",
       "        [ 0.,  5.,  6.,  0.],\n",
       "        [ 0.,  9., 10.,  0.],\n",
       "        [12.,  0.,  0., 15.]])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 把两个对角线元素放回去到指定位置\n",
    "c = t.zeros(4,4)\n",
    "c.scatter_(1, index, b.float())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对tensor的任何索引操作仍是一个tensor，想要获取标准的python对象数值，需要调用`tensor.item()`, 这个方法只对包含一个元素的tensor适用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0,0] #依旧是tensor）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0,0].item() # python float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = a[0:1, 0:1, None]\n",
    "print(d.shape)\n",
    "d.item() # 只包含一个元素的tensor即可调用tensor.item,与形状无关"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a[0].item()  ->\n",
    "# raise ValueError: only one element tensors can be converted to Python scalars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 高级索引\n",
    "PyTorch在0.2版本中完善了索引操作，目前已经支持绝大多数numpy的高级索引[^10]。高级索引可以看成是普通索引操作的扩展，但是高级索引操作的结果一般不和原始的Tensor共享内存。 \n",
    "[^10]: https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#advanced-indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2],\n",
       "         [ 3,  4,  5],\n",
       "         [ 6,  7,  8]],\n",
       "\n",
       "        [[ 9, 10, 11],\n",
       "         [12, 13, 14],\n",
       "         [15, 16, 17]],\n",
       "\n",
       "        [[18, 19, 20],\n",
       "         [21, 22, 23],\n",
       "         [24, 25, 26]]])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = t.arange(0,27).view(3,3,3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([14, 24])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[[1, 2], [1, 2], [2, 0]] # x[1,1,2]和x[2,2,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([19, 10,  1])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[[2, 1, 0], [0], [1]] # x[2,0,1],x[1,0,1],x[0,0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2],\n",
       "         [ 3,  4,  5],\n",
       "         [ 6,  7,  8]],\n",
       "\n",
       "        [[18, 19, 20],\n",
       "         [21, 22, 23],\n",
       "         [24, 25, 26]]])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[[0, 2], ...] # x[0] 和 x[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensor类型\n",
    "\n",
    "Tensor有不同的数据类型，如表3-3所示，每种类型分别对应有CPU和GPU版本(HalfTensor除外)。默认的tensor是FloatTensor，可通过`t.set_default_tensor_type` 来修改默认tensor类型(如果默认类型为GPU tensor，则所有操作都将在GPU上进行)。Tensor的类型对分析内存占用很有帮助。例如对于一个size为(1000, 1000, 1000)的FloatTensor，它有`1000*1000*1000=10^9`个元素，每个元素占32bit/8 = 4Byte内存，所以共占大约4GB内存/显存。HalfTensor是专门为GPU版本设计的，同样的元素个数，显存占用只有FloatTensor的一半，所以可以极大缓解GPU显存不足的问题，但由于HalfTensor所能表示的数值大小和精度有限[^2]，所以可能出现溢出等问题。\n",
    "\n",
    "[^2]: https://stackoverflow.com/questions/872544/what-range-of-numbers-can-be-represented-in-a-16-32-and-64-bit-ieee-754-syste\n",
    "\n",
    "表3-3: tensor数据类型\n",
    "\n",
    "| Data type                | dtype                             | CPU tensor                                                   | GPU tensor                |\n",
    "| ------------------------ | --------------------------------- | ------------------------------------------------------------ | ------------------------- |\n",
    "| 32-bit floating point    | `torch.float32` or `torch.float`  | `torch.FloatTensor`                                          | `torch.cuda.FloatTensor`  |\n",
    "| 64-bit floating point    | `torch.float64` or `torch.double` | `torch.DoubleTensor`                                         | `torch.cuda.DoubleTensor` |\n",
    "| 16-bit floating point    | `torch.float16` or `torch.half`   | `torch.HalfTensor`                                           | `torch.cuda.HalfTensor`   |\n",
    "| 8-bit integer (unsigned) | `torch.uint8`                     | [`torch.ByteTensor`](https://pytorch.org/docs/stable/tensors.html#torch.ByteTensor) | `torch.cuda.ByteTensor`   |\n",
    "| 8-bit integer (signed)   | `torch.int8`                      | `torch.CharTensor`                                           | `torch.cuda.CharTensor`   |\n",
    "| 16-bit integer (signed)  | `torch.int16` or `torch.short`    | `torch.ShortTensor`                                          | `torch.cuda.ShortTensor`  |\n",
    "| 32-bit integer (signed)  | `torch.int32` or `torch.int`      | `torch.IntTensor`                                            | `torch.cuda.IntTensor`    |\n",
    "| 64-bit integer (signed)  | `torch.int64` or `torch.long`     | `torch.LongTensor`                                           | `torch.cuda.LongTensor`   |\n",
    "\n",
    " \n",
    "\n",
    "各数据类型之间可以互相转换，`type(new_type)`是通用的做法，同时还有`float`、`long`、`half`等快捷方法。CPU tensor与GPU tensor之间的互相转换通过`tensor.cuda`和`tensor.cpu`方法实现，此外还可以使用`tensor.to(device)`。Tensor还有一个`new`方法，用法与`t.Tensor`一样，会调用该tensor对应类型的构造函数，生成与当前tensor类型一致的tensor。`torch.*_like(tensora)` 可以生成和`tensora`拥有同样属性(类型，形状，cpu/gpu)的新tensor。 `tensor.new_*(new_shape)` 新建一个不同形状的tensor。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置默认tensor，注意参数是字符串\n",
    "t.set_default_tensor_type('torch.DoubleTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float64"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.Tensor(2,3)\n",
    "a.dtype # 现在a是DoubleTensor,dtype是float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 恢复之前的默认设置\n",
    "t.set_default_tensor_type('torch.FloatTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 把a转成FloatTensor，等价于b=a.type(t.FloatTensor)\n",
    "b = a.float() \n",
    "b.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = a.type_as(b)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.new(2,3) # 等价于torch.DoubleTensor(2,3)，建议使用a.new_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.zeros_like(a) #等价于t.zeros(a.shape,dtype=a.dtype,device=a.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0],\n",
       "        [0, 0, 0]], dtype=torch.int16)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.zeros_like(a, dtype=t.int16) #可以修改某些属性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0753, 0.6563, 0.4701],\n",
       "        [0.0016, 0.4251, 0.0350]], dtype=torch.float64)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.rand_like(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1]], dtype=torch.int32)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.new_ones(4,5, dtype=t.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3., 4.], dtype=torch.float64)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.new_tensor([3,4]) # "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 逐元素操作\n",
    "\n",
    "这部分操作会对tensor的每一个元素(point-wise，又名element-wise)进行操作，此类操作的输入与输出形状一致。常用的操作如表3-4所示。\n",
    "\n",
    "表3-4: 常见的逐元素操作\n",
    "\n",
    "|函数|功能|\n",
    "|:--:|:--:|\n",
    "|abs/sqrt/div/exp/fmod/log/pow..|绝对值/平方根/除法/指数/求余/求幂..|\n",
    "|cos/sin/asin/atan2/cosh..|相关三角函数|\n",
    "|ceil/round/floor/trunc| 上取整/四舍五入/下取整/只保留整数部分|\n",
    "|clamp(input, min, max)|超过min和max部分截断|\n",
    "|sigmod/tanh..|激活函数\n",
    "\n",
    "对于很多操作，例如div、mul、pow、fmod等，PyTorch都实现了运算符重载，所以可以直接使用运算符。如`a ** 2` 等价于`torch.pow(a,2)`, `a * 2`等价于`torch.mul(a,2)`。\n",
    "\n",
    "其中`clamp(x, min, max)`的输出满足以下公式：\n",
    "$$\n",
    "y_i =\n",
    "\\begin{cases}\n",
    "min,  & \\text{if  } x_i \\lt min \\\\\n",
    "x_i,  & \\text{if  } min \\le x_i \\le max  \\\\\n",
    "max,  & \\text{if  } x_i \\gt max\\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "`clamp`常用在某些需要比较大小的地方，如取一个tensor的每个元素与另一个数的较大值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000,  0.5403, -0.4161],\n",
       "        [-0.9900, -0.6536,  0.2837]])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.arange(0, 6).view(2, 3).float()\n",
    "t.cos(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 2.],\n",
       "        [0., 1., 2.]])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a % 3 # 等价于t.fmod(a, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  4.],\n",
       "        [ 9., 16., 25.]])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a ** 2 # 等价于t.pow(a, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 2.],\n",
      "        [3., 4., 5.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[3., 3., 3.],\n",
       "        [3., 4., 5.]])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 取a中的每一个元素与3相比较大的一个 (小于3的截断成3)\n",
    "print(a)\n",
    "t.clamp(a, min=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.8415,  0.9093],\n",
       "        [ 0.1411, -0.7568, -0.9589]])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a.sin_() # 效果同 a = a.sin();b=a ,但是更高效节省显存\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  归并操作 \n",
    "此类操作会使输出形状小于输入形状，并可以沿着某一维度进行指定操作。如加法`sum`，既可以计算整个tensor的和，也可以计算tensor中每一行或每一列的和。常用的归并操作如表3-5所示。\n",
    "\n",
    "表3-5: 常用归并操作\n",
    "\n",
    "|函数|功能|\n",
    "|:---:|:---:|\n",
    "|mean/sum/median/mode|均值/和/中位数/众数|\n",
    "|norm/dist|范数/距离|\n",
    "|std/var|标准差/方差|\n",
    "|cumsum/cumprod|累加/累乘|\n",
    "\n",
    "以上大多数函数都有一个参数**`dim`**，用来指定这些操作是在哪个维度上执行的。关于dim(对应于Numpy中的axis)的解释众说纷纭，这里提供一个简单的记忆方式：\n",
    "\n",
    "假设输入的形状是(m, n, k)\n",
    "\n",
    "- 如果指定dim=0，输出的形状就是(1, n, k)或者(n, k)\n",
    "- 如果指定dim=1，输出的形状就是(m, 1, k)或者(m, k)\n",
    "- 如果指定dim=2，输出的形状就是(m, n, 1)或者(m, n)\n",
    "\n",
    "size中是否有\"1\"，取决于参数`keepdim`，`keepdim=True`会保留维度`1`。注意，以上只是经验总结，并非所有函数都符合这种形状变化方式，如`cumsum`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 2., 2.]])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = t.ones(2, 3)\n",
    "b.sum(dim = 0, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 2., 2.])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keepdim=False，不保留维度\"1\"，注意形状\n",
    "b.sum(dim=0, keepdim=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3., 3.])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  3],\n",
       "        [ 3,  7, 12]])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.arange(0, 6).view(2, 3)\n",
    "print(a)\n",
    "a.cumsum(dim=1) # 沿着行累加"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 比较\n",
    "比较函数中有一些是逐元素比较，操作类似于逐元素操作，还有一些则类似于归并操作。常用比较函数如表3-6所示。\n",
    "\n",
    "表3-6: 常用比较函数\n",
    "\n",
    "|函数|功能|\n",
    "|:--:|:--:|\n",
    "|gt/lt/ge/le/eq/ne|大于/小于/大于等于/小于等于/等于/不等|\n",
    "|topk|最大的k个数|\n",
    "|sort|排序|\n",
    "|max/min|比较两个tensor最大最小值|\n",
    "\n",
    "表中第一行的比较操作已经实现了运算符重载，因此可以使用`a>=b`、`a>b`、`a!=b`、`a==b`，其返回结果是一个`ByteTensor`，可用来选取元素。max/min这两个操作比较特殊，以max来说，它有以下三种使用情况：\n",
    "- t.max(tensor)：返回tensor中最大的一个数\n",
    "- t.max(tensor,dim)：指定维上最大的数，返回tensor和下标\n",
    "- t.max(tensor1, tensor2): 比较两个tensor相比较大的元素\n",
    "\n",
    "至于比较一个tensor和一个数，可以使用clamp函数。下面举例说明。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  3.,  6.],\n",
       "        [ 9., 12., 15.]])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.linspace(0, 15, 6).view(2, 3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[15., 12.,  9.],\n",
       "        [ 6.,  3.,  0.]])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = t.linspace(15, 0, 6).view(2, 3)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False],\n",
       "        [ True,  True,  True]])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a>b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 9., 12., 15.])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[a>b] # a中大于b的元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(15.)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.max(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([15.,  6.]),\n",
       "indices=tensor([0, 0]))"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.max(b, dim=1) \n",
    "# 第一个返回值的15和6分别表示第0行和第1行最大的元素\n",
    "# 第二个返回值的0和0表示上述最大的数是该行第0个元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[15., 12.,  9.],\n",
       "        [ 9., 12., 15.]])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.max(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10., 10., 10.],\n",
       "        [10., 12., 15.]])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 比较a和10较大的元素\n",
    "t.clamp(a, min=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 线性代数\n",
    "\n",
    "PyTorch的线性函数主要封装了Blas和Lapack，其用法和接口都与之类似。常用的线性代数函数如表3-7所示。\n",
    "\n",
    "表3-7: 常用的线性代数函数\n",
    "\n",
    "|函数|功能|\n",
    "|:---:|:---:|\n",
    "|trace|对角线元素之和(矩阵的迹)|\n",
    "|diag|对角线元素|\n",
    "|triu/tril|矩阵的上三角/下三角，可指定偏移量|\n",
    "|mm/bmm|矩阵乘法，batch的矩阵乘法|\n",
    "|addmm/addbmm/addmv/addr/badbmm..|矩阵运算\n",
    "|t|转置|\n",
    "|dot/cross|内积/外积\n",
    "|inverse|求逆矩阵\n",
    "|svd|奇异值分解\n",
    "\n",
    "具体使用说明请参见官方文档[^3]，需要注意的是，矩阵的转置会导致存储空间不连续，需调用它的`.contiguous`方法将其转为连续。\n",
    "[^3]: http://pytorch.org/docs/torch.html#blas-and-lapack-operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a.t()\n",
    "b.is_contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  9.],\n",
       "        [ 3., 12.],\n",
       "        [ 6., 15.]])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.contiguous()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Tensor和Numpy\n",
    "\n",
    "Tensor和Numpy数组之间具有很高的相似性，彼此之间的互操作也非常简单高效。需要注意的是，Numpy和Tensor共享内存。由于Numpy历史悠久，支持丰富的操作，所以当遇到Tensor不支持的操作时，可先转成Numpy数组，处理后再转回tensor，其转换开销很小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1.],\n",
       "       [1., 1., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.ones([2, 3],dtype=np.float32)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = t.from_numpy(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = t.Tensor(a) # 也可以直接将numpy对象传入Tensor\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  1., 100.,   1.],\n",
       "        [  1.,   1.,   1.]])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0, 1]=100\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1., 100.,   1.],\n",
       "       [  1.,   1.,   1.]], dtype=float32)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = b.numpy() # a, b, c三个对象共享内存\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意**： 当numpy的数据类型和Tensor的类型不一样的时候，数据会被复制，不会共享内存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.ones([2, 3])\n",
    "# 注意和上面的a的区别（dtype不是float32）\n",
    "a.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = t.Tensor(a) # 此处进行拷贝，不共享内存\n",
    "b.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = t.from_numpy(a) # 注意c的类型（DoubleTensor）\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0, 1] = 100\n",
    "b # b与a不共享内存，所以即使a改变了，b也不变"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  1., 100.,   1.],\n",
       "        [  1.,   1.,   1.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c # c与a共享内存"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意：** 不论输入的类型是什么，t.tensor都会进行数据拷贝，不会共享内存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = t.tensor(a) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1., 100.,   1.],\n",
       "       [  1.,   1.,   1.]])"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor[0,0]=0\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "广播法则(broadcast)是科学运算中经常使用的一个技巧，它在快速执行向量化的同时不会占用额外的内存/显存。\n",
    "Numpy的广播法则定义如下：\n",
    "\n",
    "- 让所有输入数组都向其中shape最长的数组看齐，shape中不足的部分通过在前面加1补齐\n",
    "- 两个数组要么在某一个维度的长度一致，要么其中一个为1，否则不能计算 \n",
    "- 当输入数组的某个维度的长度为1时，计算时沿此维度复制扩充成一样的形状\n",
    "\n",
    "PyTorch当前已经支持了自动广播法则，但是笔者还是建议读者通过以下两个函数的组合手动实现广播法则，这样更直观，更不易出错：\n",
    "\n",
    "- `unsqueeze`或者`view`，或者tensor[None],：为数据某一维的形状补1，实现法则1\n",
    "- `expand`或者`expand_as`，重复数组，实现法则3；该操作不会复制数组，所以不会占用额外的空间。\n",
    "\n",
    "注意，repeat实现与expand相类似的功能，但是repeat会把相同数据复制多份，因此会占用额外的空间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = t.ones(3, 2)\n",
    "b = t.zeros(2, 3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1.],\n",
       "         [1., 1.],\n",
       "         [1., 1.]],\n",
       "\n",
       "        [[1., 1.],\n",
       "         [1., 1.],\n",
       "         [1., 1.]]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 自动广播法则\n",
    "# 第一步：a是2维,b是3维，所以先在较小的a前面补1 ，\n",
    "#               即：a.unsqueeze(0)，a的形状变成（1，3，2），b的形状是（2，3，1）,\n",
    "# 第二步:   a和b在第一维和第三维形状不一样，其中一个为1 ，\n",
    "#               可以利用广播法则扩展，两个形状都变成了（2，3，2）\n",
    "a+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1.],\n",
       "         [1., 1.],\n",
       "         [1., 1.]],\n",
       "\n",
       "        [[1., 1.],\n",
       "         [1., 1.],\n",
       "         [1., 1.]]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 手动广播法则\n",
    "# 或者 a.view(1,3,2).expand(2,3,2)+b.expand(2,3,2)\n",
    "a[None].expand(2, 3, 2) + b.expand(2,3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expand不会占用额外空间，只会在需要的时候才扩充，可极大节省内存\n",
    "e = a.unsqueeze(0).expand(10000000000000, 3,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3   \n",
    "\n",
    "tensor的数据结构如图3-1所示。tensor分为头信息区(Tensor)和存储区(Storage)，信息区主要保存着tensor的形状（size）、步长（stride）、数据类型（type）等信息，而真正的数据则保存成连续数组。由于数据动辄成千上万，因此信息区元素占用内存较少，主要内存占用则取决于tensor中元素的数目，也即存储区的大小。\n",
    "\n",
    "一般来说一个tensor有着与之相对应的storage, storage是在data之上封装的接口，便于使用，而不同tensor的头信息一般不同，但却可能使用相同的数据。下面看两个例子。\n",
    "\n",
    "![图3-1: Tensor的数据结构](imgs/tensor_data_structure.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0\n",
       " 1\n",
       " 2\n",
       " 3\n",
       " 4\n",
       " 5\n",
       "[torch.storage.TypedStorage(dtype=torch.int64, device=cpu) of size 6]"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.arange(0, 6)\n",
    "a.storage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0\n",
       " 1\n",
       " 2\n",
       " 3\n",
       " 4\n",
       " 5\n",
       "[torch.storage.TypedStorage(dtype=torch.int64, device=cpu) of size 6]"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a.view(2, 3)\n",
    "b.storage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 一个对象的id值可以看作它在内存中的地址\n",
    "# storage的内存地址一样，即是同一个storage\n",
    "id(b.storage()) == id(a.storage())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0, 100,   2],\n",
       "        [  3,   4,   5]])"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a改变，b也随之改变，因为他们共享storage\n",
    "a[1] = 100\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0\n",
       " 100\n",
       " 2\n",
       " 3\n",
       " 4\n",
       " 5\n",
       "[torch.storage.TypedStorage(dtype=torch.int64, device=cpu) of size 6]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = a[2:] \n",
    "c.storage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1701262261392, 1701262261376)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.data_ptr(), a.data_ptr() # data_ptr返回tensor首元素的内存地址\n",
    "# 可以看出相差16，这是因为2*8=16--相差两个元素，每个元素占8个字节(int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   0,  100, -100,    3,    4,    5])"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[0] = -100 # c[0]的内存地址对应a[2]的内存地址\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6666,  100, -100],\n",
       "        [   3,    4,    5]])"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = t.LongTensor(c.storage())\n",
    "d[0] = 6666\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 下面４个tensor共享storage\n",
    "id(a.storage()) == id(b.storage()) == id(c.storage()) == id(d.storage())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 2, 0)"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.storage_offset(), c.storage_offset(), d.storage_offset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = b[::2, ::2] # 隔2行/列取一个元素\n",
    "id(e.storage()) == id(a.storage())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 1), (6, 2))"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.stride(), e.stride()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 6666\n",
       " 100\n",
       " -100\n",
       " 3\n",
       " 4\n",
       " 5\n",
       "[torch.storage.TypedStorage(dtype=torch.int64, device=cpu) of size 6]"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.is_contiguous()\n",
    "e.storage()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可见绝大多数操作并不修改tensor的数据，而只是修改了tensor的头信息。这种做法更节省内存，同时提升了处理速度。在使用中需要注意。\n",
    "此外有些操作会导致tensor不连续，这时需调用`tensor.contiguous`方法将它们变成连续的数据，该方法与原来的数据共享storage。\n",
    "另外读者可以思考一下，之前说过的高级索引一般不共享stroage，而普通索引共享storage，这是为什么？（提示：普通索引可以通过只修改tensor的offset，stride和size，而不修改storage来实现）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.4 其它有关Tensor的话题\n",
    "这部分的内容不好专门划分一小节，但是笔者认为仍值得读者注意，故而将其放在这一小节。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPU/CPU\n",
    "tensor可以很随意的在gpu/cpu上传输。使用`tensor.cuda(device_id)`或者`tensor.cpu()`。另外一个更通用的方法是`tensor.to(device)`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.randn(3, 4)\n",
    "a.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if t.cuda.is_available():\n",
    "    a = t.randn(3,4, device=t.device('cuda:0'))\n",
    "    # 等价于\n",
    "    # a.t.randn(3,4).cuda(0)\n",
    "    # 但是前者更快\n",
    "a.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.7164, -0.0035, -0.9862, -0.2198],\n",
       "        [-2.9509,  0.6435,  1.1086,  0.3062],\n",
       "        [-0.3360,  0.9184, -0.6019,  0.4407]])"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = t.device('cpu')\n",
    "a.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意**\n",
    "- 尽量使用`tensor.to(device)`, 将`device`设为一个可配置的参数，这样可以很轻松的使程序同时兼容GPU和CPU\n",
    "- 数据在GPU之中传输的速度要远快于内存(CPU)到显存(GPU), 所以尽量避免频繁的在内存和显存中传输数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 持久化\n",
    "Tensor的保存和加载十分的简单，使用t.save和t.load即可完成相应的功能。在save/load时可指定使用的`pickle`模块，在load时还可将GPU tensor映射到CPU或其它GPU上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.7164, -0.0035, -0.9862, -0.2198],\n",
      "        [-2.9509,  0.6435,  1.1086,  0.3062],\n",
      "        [-0.3360,  0.9184, -0.6019,  0.4407]], device='cuda:0')\n",
      "tensor([[-1.7164, -0.0035, -0.9862, -0.2198],\n",
      "        [-2.9509,  0.6435,  1.1086,  0.3062],\n",
      "        [-0.3360,  0.9184, -0.6019,  0.4407]])\n"
     ]
    }
   ],
   "source": [
    "if t.cuda.is_available():\n",
    "    a = a.cuda(0) # 把a转为GPU0上的tensor,\n",
    "    t.save(a,'a.pth')\n",
    "\n",
    "    # 加载为b, 存储于GPU0上(因为保存时tensor就在GPU0上)\n",
    "    b = t.load('a.pth')\n",
    "    print(b)\n",
    "    # 加载为c, 存储于CPU\n",
    "    c = t.load('a.pth', map_location=lambda storage, loc: storage)\n",
    "    print(c)\n",
    "    # 加载为d, 存储于GPU1上\n",
    "    # d = t.load('a.pth', map_location={'cuda:0':'cuda:1'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####   向量化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "向量化计算是一种特殊的并行计算方式，相对于一般程序在同一时间只执行一个操作的方式，它可在同一时间执行多个操作，通常是对不同的数据执行同样的一个或一批指令，或者说把指令应用于一个数组/向量上。向量化可极大提高科学运算的效率，Python本身是一门高级语言，使用很方便，但这也意味着很多操作很低效，尤其是`for`循环。在科学计算程序中应当极力避免使用Python原生的`for循环`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def for_loop_add(x, y):\n",
    "    result = []\n",
    "    for i,j in zip(x, y):\n",
    "        result.append(i + j)\n",
    "    return t.Tensor(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "419 µs ± 82.1 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "2.12 µs ± 631 ns per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "x = t.zeros(100)\n",
    "y = t.ones(100)\n",
    "%timeit -n 10 for_loop_add(x, y)\n",
    "%timeit -n 10 x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可见二者有超过几十倍的速度差距，因此在实际使用中应尽量调用内建函数(buildin-function)，这些函数底层由C/C++实现，能通过执行底层优化实现高效计算。因此在平时写代码时，就应养成向量化的思维习惯，千万避免对较大的tensor进行逐元素遍历。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此外还有以下几点需要注意：\n",
    "- 大多数`t.function`都有一个参数`out`，这时候产生的结果将保存在out指定tensor之中。\n",
    "- `t.set_num_threads`可以设置PyTorch进行CPU多线程并行计算时候所占用的线程数，这个可以用来限制PyTorch所占用的CPU数目。\n",
    "- `t.set_printoptions`可以用来设置打印tensor时的数值精度和格式。\n",
    "下面举例说明。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int32\n",
      "tensor(19999999, dtype=torch.int32) tensor(19999998, dtype=torch.int32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(19999999), tensor(19999998))"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.arange(0, 20000000,dtype=t.int16)\n",
    "print(a.dtype)\n",
    "print(a[-1], a[-2]) # 16bit的IntTensor精度有限导致溢出\n",
    "b = t.LongTensor()\n",
    "t.arange(0, 20000000, out=b) # 64bit的LongTensor不会溢出\n",
    "b[-1],b[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2280,  0.7623,  0.2666],\n",
       "        [ 1.2862, -0.2505, -1.7041]])"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.randn(2,3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2279753536,  0.7623047829,  0.2666030526],\n",
       "        [ 1.2861752510, -0.2505084276, -1.7040781975]])"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.set_printoptions(precision=10)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.5 小试牛刀：线性回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "线性回归是机器学习入门知识，应用十分广泛。线性回归利用数理统计中回归分析，来确定两种或两种以上变量间相互依赖的定量关系的，其表达形式为$y = wx+b+e$，$e$为误差服从均值为0的正态分布。首先让我们来确认线性回归的损失函数：\n",
    "$$\n",
    "loss = \\sum_i^N \\frac 1 2 ({y_i-(wx_i+b)})^2\n",
    "$$\n",
    "然后利用随机梯度下降法更新参数$\\textbf{w}$和$\\textbf{b}$来最小化损失函数，最终学得$\\textbf{w}$和$\\textbf{b}$的数值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "device = t.device('cpu') #如果你想用gpu，改成t.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置随机数种子，保证在不同电脑上运行时下面的输出一致\n",
    "t.manual_seed(1000) \n",
    "\n",
    "def get_fake_data(batch_size=8):\n",
    "    ''' 产生随机数据：y=x*2+3，加上了一些噪声'''\n",
    "    x = t.rand(batch_size, 1, device=device) * 5\n",
    "    y = x * 2 + 3 +  t.randn(batch_size, 1, device=device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x18c62924610>"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 来看看产生的x-y分布\n",
    "x, y = get_fake_data(batch_size=16)\n",
    "plt.scatter(x.squeeze().cpu().numpy(), y.squeeze().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w:  1.9115010499954224 b:  3.044184446334839\n"
     ]
    }
   ],
   "source": [
    "# 随机初始化参数\n",
    "w = t.rand(1, 1).to(device)\n",
    "b = t.zeros(1, 1).to(device)\n",
    "\n",
    "lr =0.02 # 学习率\n",
    "\n",
    "for ii in range(500):\n",
    "    x, y = get_fake_data(batch_size=4)\n",
    "    \n",
    "    # forward：计算loss\n",
    "    y_pred = x.mm(w) + b.expand_as(y) # x@W等价于x.mm(w);for python3 only\n",
    "    loss = 0.5 * (y_pred - y) ** 2 # 均方误差\n",
    "    loss = loss.mean()\n",
    "    \n",
    "    # backward：手动计算梯度\n",
    "    dloss = 1\n",
    "    dy_pred = dloss * (y_pred - y)\n",
    "    \n",
    "    dw = x.t().mm(dy_pred)\n",
    "    db = dy_pred.sum()\n",
    "    \n",
    "    # 更新参数\n",
    "    w.sub_(lr * dw)\n",
    "    b.sub_(lr * db)\n",
    "    \n",
    "    if ii%50 ==0:\n",
    "       \n",
    "        # 画图\n",
    "        display.clear_output(wait=True)\n",
    "        x = t.arange(0, 6).view(-1, 1)\n",
    "        y = x.float().mm(w) + b.expand_as(x)\n",
    "        plt.plot(x.cpu().numpy(), y.cpu().numpy()) # predicted\n",
    "        \n",
    "        x2, y2 = get_fake_data(batch_size=32) \n",
    "        plt.scatter(x2.numpy(), y2.numpy()) # true data\n",
    "        \n",
    "        plt.xlim(0, 5)\n",
    "        plt.ylim(0, 13)\n",
    "        plt.show()\n",
    "        plt.pause(0.5)\n",
    "        \n",
    "print('w: ', w.item(), 'b: ', b.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可见程序已经基本学出w=2、b=3，并且图中直线和数据已经实现较好的拟合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "虽然上面提到了许多操作，但是只要掌握了这个例子基本上就可以了，其他的知识，读者日后遇到的时候，可以再看看这部份的内容或者查找对应文档。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "all_have",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
